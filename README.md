# `icp_huggingface`

This is a DeAI ICP project for using Huggingface models for inference. It is a simple example of how to use the Huggingface crate `Candle` to perform inference on a model. It uses a Text To Speech(TTS) model to generate speech from text.

# `Steps`

1. Manually download any safetensor file from any transformer model on hugging face depending on your use case. The file can be found in the file section of the huggingface models.  
2. Refer to this [github repo](https://github.com/dfinity/examples/blob/master/rust/face-recognition/README.md) on how to upload model files to the canister. This uses a CLI to upload the model file in the canister in chunks. 
3. Clone the repo. 
4. Replace your filename in the lib.rs file. 
5. Run the command `cargo build --release --target wasm32-unknown-unknown --<your-backend-canister>`
6. Run the command `candid-extractor target/wasm32-unknown-unknown/release/icp_huggingface_backend.wasm > src/icp_huggingface_backend/icp_huggingface__backend.did`.

7. `dfx start` then `dfx deploy`.
N/B: Before uploading your model file in chunks, you need to create a new identity using the command: `dfx identity new <identity> --storage-mode plaintext`

Crates in the Cargo.toml file in  the backend have a `getrandom` dependency which is not supported by the `wasm32-unknown-unknown` target. This is why we create a new `.cargo/config.toml file` in the root directory of the project. 

Of note is also the `dfx.json file` which has changes to accomodate the `getrandom` crate. 

To learn more before you start working with `icp_huggingface`, see the following documentation available online:

- [Quick Start](https://internetcomputer.org/docs/current/developer-docs/setup/deploy-locally)
- [SDK Developer Tools](https://internetcomputer.org/docs/current/developer-docs/setup/install)
- [Rust Canister Development Guide](https://internetcomputer.org/docs/current/developer-docs/backend/rust/)
- [ic-cdk](https://docs.rs/ic-cdk)
- [ic-cdk-macros](https://docs.rs/ic-cdk-macros)
- [Candid Introduction](https://internetcomputer.org/docs/current/developer-docs/backend/candid/)
- [HuggingFace](https://huggingface.co/learn/ml-games-course/en/unit1/what-is-hf)
- [Transformer models](https://huggingface.co/docs/transformers/en/index)
- [Candle](https://github.com/huggingface/candle)


If you want to start working on your project right away, you might want to try the following commands:

```bash
cd icp_huggingface/
dfx help
dfx canister --help
```

## Running the project locally

If you want to test your project locally, you can use the following commands:

```bash
# Starts the replica, running in the background
dfx start --background

# Deploys your canisters to the replica and generates your candid interface
dfx deploy
```

Once the job completes, your application will be available at `http://localhost:4943?canisterId={asset_canister_id}`.

If you have made changes to your backend canister, you can generate a new candid interface with

```bash
npm run generate
```

at any time. This is recommended before starting the frontend development server, and will be run automatically any time you run `dfx deploy`.

If you are making frontend changes, you can start a development server with

```bash
npm start
```

Which will start a server at `http://localhost:8080`, proxying API requests to the replica at port 4943.

### Note on frontend environment variables

If you are hosting frontend code somewhere without using DFX, you may need to make one of the following adjustments to ensure your project does not fetch the root key in production:

- set`DFX_NETWORK` to `ic` if you are using Webpack
- use your own preferred method to replace `process.env.DFX_NETWORK` in the autogenerated declarations
  - Setting `canisters -> {asset_canister_id} -> declarations -> env_override to a string` in `dfx.json` will replace `process.env.DFX_NETWORK` with the string in the autogenerated declarations
- Write your own `createActor` constructor
